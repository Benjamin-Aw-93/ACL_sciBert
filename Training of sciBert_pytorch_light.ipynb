{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "# Reading in files\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Torch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast as AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Lightning modules\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics.functional import accuracy, auroc\n",
    "from torchmetrics import F1Score\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Split dataset/validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/csv/train.csv\")\n",
    "val_df = pd.read_csv(\"data/csv/test.csv\")\n",
    "test_df = pd.read_csv(\"data/csv/dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>string</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>However, how frataxin interacts with the Fe-S ...</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the study by Hickey et al. (2012), spikes w...</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The drug also reduces catecholamine secretion,...</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>By clustering with lowly aggressive close kin ...</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ophthalmic symptoms are rare manifestations of...</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8238</th>\n",
       "      <td>Importantly, the results of Pascalis et al. (2...</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8239</th>\n",
       "      <td>As suggested by Nguena et al, there is a need ...</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8240</th>\n",
       "      <td>Skeletal muscle is also a primary site of dise...</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8241</th>\n",
       "      <td>ACTIVATION OF TRANSCRIPTION FACTORS Roles for ...</td>\n",
       "      <td>method</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8242</th>\n",
       "      <td>Most studies focused on the relation between b...</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8243 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 string       label\n",
       "0     However, how frataxin interacts with the Fe-S ...  background\n",
       "1     In the study by Hickey et al. (2012), spikes w...  background\n",
       "2     The drug also reduces catecholamine secretion,...  background\n",
       "3     By clustering with lowly aggressive close kin ...  background\n",
       "4     Ophthalmic symptoms are rare manifestations of...  background\n",
       "...                                                 ...         ...\n",
       "8238  Importantly, the results of Pascalis et al. (2...  background\n",
       "8239  As suggested by Nguena et al, there is a need ...  background\n",
       "8240  Skeletal muscle is also a primary site of dise...  background\n",
       "8241  ACTIVATION OF TRANSCRIPTION FACTORS Roles for ...      method\n",
       "8242  Most studies focused on the relation between b...  background\n",
       "\n",
       "[8243 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIBERT_MODEL_NAME = 'allenai/scibert_scivocab_uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(SCIBERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_row = train_df.iloc[16]\n",
    "sample_comment = sample_row[\"string\"]\n",
    "sample_labels = sample_row[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_idx = {\"method\":0, \"background\": 1, \"result\": 2}\n",
    "idx_to_label = {0:\"method\",  1:\"background\", 2:\"result\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_idx[sample_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_TOKEN_COUNT = 512\n",
    "\n",
    "encoding = tokenizer.encode_plus(\n",
    "  sample_comment,\n",
    "  add_special_tokens = True,\n",
    "  max_length= MAX_TOKEN_COUNT,\n",
    "  return_token_type_ids = False,\n",
    "  padding = \"max_length\",\n",
    "  return_attention_mask = True,\n",
    "  return_tensors = 'pt',\n",
    ")\n",
    "\n",
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SciBertDataset(Dataset):\n",
    "  \n",
    "  def __init__(self, data: pd.DataFrame, tokenizer: AutoTokenizer, max_token_len: int = 512, mapping = label_to_idx):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.data = data\n",
    "    self.max_token_len = max_token_len\n",
    "    self.mapping = mapping\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "  \n",
    "  def __getitem__(self, index: int):\n",
    "    \n",
    "    data_row = self.data.iloc[index]\n",
    "    text = data_row[\"string\"]\n",
    "    labels = self.mapping[data_row[\"label\"]]\n",
    "    \n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_token_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding=\"max_length\",\n",
    "      truncation=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    return dict(\n",
    "      text=text,\n",
    "      input_ids=encoding[\"input_ids\"].flatten(),\n",
    "      attention_mask=encoding[\"attention_mask\"].flatten(),\n",
    "      labels=labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SciBertDataset(train_df, tokenizer, max_token_len = 512, mapping = label_to_idx)\n",
    "val_dataset = SciBertDataset(val_df, tokenizer, max_token_len = 512, mapping = label_to_idx)\n",
    "test_dataset = SciBertDataset(test_df, tokenizer, max_token_len = 512, mapping = label_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SciBertDataModule(pl.LightningDataModule):\n",
    "  def __init__(self, train_df, test_df, val_df, tokenizer, batch_size=8, max_token_len=512):\n",
    "    \n",
    "    super().__init__()\n",
    "    self.batch_size = batch_size\n",
    "    self.train_df = train_df\n",
    "    self.test_df = test_df\n",
    "    self.val_df = val_df\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_token_len = max_token_len\n",
    "  \n",
    "  def setup(self, stage=None):\n",
    "    self.train_dataset = SciBertDataset(\n",
    "      self.train_df,\n",
    "      self.tokenizer,\n",
    "      self.max_token_len\n",
    "    )\n",
    "    self.test_dataset = SciBertDataset(\n",
    "      self.test_df,\n",
    "      self.tokenizer,\n",
    "      self.max_token_len\n",
    "    )\n",
    "    self.val_dataset = SciBertDataset(\n",
    "      self.val_df,\n",
    "      self.tokenizer,\n",
    "      self.max_token_len\n",
    "    )\n",
    "  \n",
    "  \n",
    "  def train_dataloader(self):\n",
    "    return DataLoader(\n",
    "      self.train_dataset,\n",
    "      batch_size=self.batch_size,\n",
    "      shuffle=True\n",
    "    )\n",
    "  \n",
    "  def val_dataloader(self):\n",
    "    return DataLoader(\n",
    "      self.test_dataset,\n",
    "      batch_size=self.batch_size\n",
    "    )\n",
    "  \n",
    "  def test_dataloader(self):\n",
    "    return DataLoader(\n",
    "      self.test_dataset,\n",
    "      batch_size=self.batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 2\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "data_module = SciBertDataModule(\n",
    "  train_df,\n",
    "  test_df,\n",
    "  val_df,\n",
    "  tokenizer,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  max_token_len=MAX_TOKEN_COUNT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['However, how frataxin interacts with the Fe-S cluster biosynthesis components remains unclear as direct one-to-one interactions with each component were reported (IscS [12,22], IscU/Isu1 [6,11,16] or ISD11/Isd11 [14,15]).',\n",
       "  'In the study by Hickey et al. (2012), spikes were sampled from the field at the point of physiological\\nrobinson et al.: genomic regions influencing root traits in barley 11 of 13\\nmaturity, dried, grain threshed by hand, and stored at âˆ’20C to preserve grain dormancy before germination testing.',\n",
       "  'The drug also reduces catecholamine secretion, thereby reducing stress and leading to a modest (10-20%) reduction in heart rate and blood pressure, which may be particularly beneficial in patients with cardiovascular disease.(7) Unlike midazolam, dexmedetomidine does not affect the ventilatory response to carbon dioxide.',\n",
       "  'By clustering with lowly aggressive close kin (King 1989a,b; Viblanc et al. 2010; Arnaud, Dobson & Murie 2012), breeding females may decrease the time/energy cost of maintaining territorial boundaries (Festa-Bianchet & Boag 1982; Murie & Harris 1988), which could ultimately lead to increases in net energy income (TA) or higher allocations in somatic or reproductive functions.',\n",
       "  'Ophthalmic symptoms are rare manifestations of the intracranial arachnoid cyst, and include unilateral exophthalmos, visual field abnormality, decreased visual acuity and isolated palsies of the third, fourth and sixth cranial nerves [1â€“5].',\n",
       "  'Recent studies identified Wee1 as a potential molecular target in cancer cells and the selective small molecule Wee1-inhibitor MK-1775 demonstrated promising results in cancer cells with enhanced levels of Wee1 (96-98).',\n",
       "  'These problems combine to make early diagnosis essential and immediate treatment a necessity, even for the youngest patients [17, 18].',\n",
       "  'Also, results demonstrated that the molecular weight and G/M ratio were important factors in controlling the antioxidant properties of sodium alginate (Åžen 2011).'],\n",
       " 'input_ids': tensor([[ 102,  694,  422,  ...,    0,    0,    0],\n",
       "         [ 102,  121,  111,  ...,    0,    0,    0],\n",
       "         [ 102,  111, 1698,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 102, 2151,  826,  ...,    0,    0,    0],\n",
       "         [ 102,  407, 2010,  ...,    0,    0,    0],\n",
       "         [ 102,  469,  422,  ...,    0,    0,    0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = next(iter(DataLoader(train_dataset, batch_size=8)))\n",
    "sample_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIBERT_MODEL_NAME = \"allenai/scibert_scivocab_uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SciBertClassModel(pl.LightningModule):\n",
    "  \n",
    "  def __init__(self, n_classes: int, n_training_steps=None, n_warmup_steps=None):\n",
    "    \n",
    "    super().__init__()\n",
    "    self.scibert = AutoModel.from_pretrained(SCIBERT_MODEL_NAME, return_dict=True)\n",
    "    self.classifier = nn.Linear(self.scibert.config.hidden_size, n_classes)\n",
    "    self.n_training_steps = n_training_steps\n",
    "    self.n_warmup_steps = n_warmup_steps\n",
    "    self.criterion = nn.CrossEntropyLoss()\n",
    "  \n",
    "  def forward(self, input_ids, attention_mask, labels=None):\n",
    "    output = self.scibert(input_ids, attention_mask=attention_mask)\n",
    "    output = self.classifier(output.pooler_output)\n",
    "    loss = 0\n",
    "    if labels is not None:\n",
    "        loss = self.criterion(output, labels)\n",
    "    return loss, output\n",
    "  \n",
    "  def training_step(self, batch, batch_idx):\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    loss, outputs = self(input_ids, attention_mask, labels)\n",
    "    self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "    return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "  \n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    loss, outputs = self(input_ids, attention_mask, labels)\n",
    "    self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "    return loss\n",
    "  \n",
    "  def test_step(self, batch, batch_idx):\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    loss, outputs = self(input_ids, attention_mask, labels)\n",
    "    self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "    return loss\n",
    "  \n",
    "  def training_epoch_end(self, outputs):\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    \n",
    "    for output in outputs:\n",
    "      for out_labels in output[\"labels\"].detach().cpu():\n",
    "        labels.append(out_labels)\n",
    "      for out_predictions in output[\"predictions\"].detach().cpu():\n",
    "        predictions.append(out_predictions)\n",
    "    \n",
    "    labels = torch.stack(labels).int()\n",
    "    predictions = torch.stack(predictions)\n",
    "\n",
    "    print(labels)\n",
    "    print(predictions)\n",
    "    \n",
    "    for i, name in enumerate(LABEL_COLUMNS):\n",
    "      class_roc_auc = auroc(predictions[:, i], labels[:, i])\n",
    "      self.logger.experiment.add_scalar(f\"{name}_roc_auc/Train\", class_roc_auc, self.current_epoch)\n",
    "  \n",
    "  def configure_optimizers(self):\n",
    "    optimizer = AdamW(self.parameters(), lr=2e-5)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "      optimizer,\n",
    "      num_warmup_steps=self.n_warmup_steps,\n",
    "      num_training_steps=self.n_training_steps\n",
    "    )\n",
    "    return dict(\n",
    "      optimizer = optimizer,\n",
    "      lr_scheduler = dict(\n",
    "        scheduler = scheduler,\n",
    "        interval = 'step'\n",
    "      )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "  dirpath = \"checkpoints\",\n",
    "  filename = \"best-checkpoint\",\n",
    "  save_top_k = 1,\n",
    "  verbose = True,\n",
    "  monitor = \"val_loss\",\n",
    "  mode = \"min\"\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"sciBert\")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "  logger = logger,\n",
    "  callbacks = [checkpoint_callback, early_stopping_callback],\n",
    "  max_epochs = N_EPOCHS,\n",
    "  accelerator = \"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(412, 2060)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch=len(train_df) // BATCH_SIZE\n",
    "total_training_steps = steps_per_epoch * N_EPOCHS\n",
    "warmup_steps = total_training_steps // 5\n",
    "warmup_steps, total_training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SciBertClassModel(\n",
    "  n_classes = 3,\n",
    "  n_warmup_steps = warmup_steps,\n",
    "  n_training_steps = total_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\Benjamin Aw\\Desktop\\SciBERT\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name       | Type             | Params\n",
      "------------------------------------------------\n",
      "0 | scibert    | BertModel        | 109 M \n",
      "1 | classifier | Linear           | 2.3 K \n",
      "2 | criterion  | CrossEntropyLoss | 0     \n",
      "------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "439.683   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76728199e96e46999e6c12da6437bdf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Benjamin Aw\\Desktop\\SciBERT\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:225: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Benjamin Aw\\Desktop\\SciBERT\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:225: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382ddd4393124a4dbdda12fd8c00cf71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ea082b6a79407c852eb16e8ce1ab06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1,  ..., 1, 2, 1], dtype=torch.int32)\n",
      "tensor([[ 0.5595,  0.5880, -0.2772],\n",
      "        [ 0.4827,  0.6871, -0.4410],\n",
      "        [ 0.3551,  0.4746, -0.4078],\n",
      "        ...,\n",
      "        [ 1.9280,  1.7423, -2.5922],\n",
      "        [-1.5197, -0.4256,  1.1477],\n",
      "        [-0.2428,  3.1069, -2.0503]])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LABEL_COLUMNS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Benjamin Aw\\Desktop\\SciBERT\\Training of sciBert_pytorch_light.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Benjamin%20Aw/Desktop/SciBERT/Training%20of%20sciBert_pytorch_light.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, data_module)\n",
      "File \u001b[1;32mc:\\Users\\Benjamin Aw\\Desktop\\SciBERT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:696\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    693\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    695\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[1;32m--> 696\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    697\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    698\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Benjamin Aw\\Desktop\\SciBERT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    649\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    651\u001b[0m \u001b[39m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[1;32mc:\\Users\\Benjamin Aw\\Desktop\\SciBERT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:735\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    731\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    732\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[0;32m    733\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    734\u001b[0m )\n\u001b[1;32m--> 735\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[0;32m    737\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    738\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Benjamin Aw\\Desktop\\SciBERT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1166\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[0;32m   1164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m-> 1166\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m   1168\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[1;32mc:\\Users\\Benjamin Aw\\Desktop\\SciBERT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1252\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m   1251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[1;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[1;32mc:\\Users\\Benjamin Aw\\Desktop\\SciBERT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1283\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1280\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[0;32m   1282\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1283\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32mc:\\Users\\Benjamin Aw\\Desktop\\SciBERT\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:201\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 201\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_advance_end()\n\u001b[0;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Benjamin Aw\\Desktop\\SciBERT\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:287\u001b[0m, in \u001b[0;36mFitLoop.on_advance_end\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    280\u001b[0m epoch_end_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch_loop\u001b[39m.\u001b[39m_prepare_outputs_training_epoch_end(\n\u001b[0;32m    281\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs,\n\u001b[0;32m    282\u001b[0m     lightning_module\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m    283\u001b[0m     num_optimizers\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizers),\n\u001b[0;32m    284\u001b[0m )\n\u001b[0;32m    285\u001b[0m \u001b[39m# run lightning module hook training_epoch_end\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[39m# refresh the result for custom logging at the epoch level\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m epoch_end_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\u001b[39m\"\u001b[39;49m\u001b[39mtraining_epoch_end\u001b[39;49m\u001b[39m\"\u001b[39;49m, epoch_end_outputs)\n\u001b[0;32m    288\u001b[0m \u001b[39mif\u001b[39;00m epoch_end_outputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    289\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\n\u001b[0;32m    290\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`training_epoch_end` expects a return of None. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    291\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mHINT: remove the return statement in `training_epoch_end`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    292\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Benjamin Aw\\Desktop\\SciBERT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1550\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[1;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1547\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[0;32m   1549\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1550\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1552\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "\u001b[1;32mc:\\Users\\Benjamin Aw\\Desktop\\SciBERT\\Training of sciBert_pytorch_light.ipynb Cell 23\u001b[0m in \u001b[0;36mSciBertClassModel.training_epoch_end\u001b[1;34m(self, outputs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benjamin%20Aw/Desktop/SciBERT/Training%20of%20sciBert_pytorch_light.ipynb#X35sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mprint\u001b[39m(labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benjamin%20Aw/Desktop/SciBERT/Training%20of%20sciBert_pytorch_light.ipynb#X35sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mprint\u001b[39m(predictions)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Benjamin%20Aw/Desktop/SciBERT/Training%20of%20sciBert_pytorch_light.ipynb#X35sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, name \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(LABEL_COLUMNS):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benjamin%20Aw/Desktop/SciBERT/Training%20of%20sciBert_pytorch_light.ipynb#X35sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m   class_roc_auc \u001b[39m=\u001b[39m auroc(predictions[:, i], labels[:, i])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benjamin%20Aw/Desktop/SciBERT/Training%20of%20sciBert_pytorch_light.ipynb#X35sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mexperiment\u001b[39m.\u001b[39madd_scalar(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m_roc_auc/Train\u001b[39m\u001b[39m\"\u001b[39m, class_roc_auc, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_epoch)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LABEL_COLUMNS' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sciBert",
   "language": "python",
   "name": "scibert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b110531817a2bf0b8e1fa59ad47ae74cf2e8602bdaf1b8cf6b96ebd8cf78ed6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
